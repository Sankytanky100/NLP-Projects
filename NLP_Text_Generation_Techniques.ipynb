{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sankytanky100/NLP-Projects/blob/main/NLP_Text_Generation_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Text Processing"
      ],
      "metadata": {
        "id": "JbXvRfAxN13P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "\n",
        "def get_part_of_speech(word):\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "\n",
        "  pos_counts = Counter()\n",
        "\n",
        "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
        "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
        "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
        "\n",
        "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "  return most_likely_part_of_speech"
      ],
      "metadata": {
        "id": "sZ7wgex2NzII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "LVD5uBdiW03M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "from np_chunk_counter import np_chunk_counter\n",
        "\n",
        "# define noun-phrase chunk grammar here\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# create a list to hold noun-phrase chunked sentences\n",
        "np_chunked_oz = list()\n",
        "\n",
        "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
        "for pos_tagged_sentence in pos_tagged_oz:\n",
        "  # chunk each sentence and append to np_chunked_oz here\n",
        "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
        "\n",
        "# store and print the most common np-chunks here\n",
        "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
        "print(most_common_np_chunks)\n"
      ],
      "metadata": {
        "id": "6vGpWq1wW0W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunk Counter"
      ],
      "metadata": {
        "id": "xUwIRAgtXASW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
        "def np_chunk_counter(chunked_sentences):\n",
        "\n",
        "    # create a list to hold chunks\n",
        "    chunks = list()\n",
        "\n",
        "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
        "    for chunked_sentence in chunked_sentences:\n",
        "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "            chunks.append(tuple(subtree))\n",
        "\n",
        "    # create a Counter object\n",
        "    chunk_counter = Counter()\n",
        "\n",
        "    # for-loop through the list of chunks\n",
        "    for chunk in chunks:\n",
        "        # increase counter of specific chunk by 1\n",
        "        chunk_counter[chunk] += 1\n",
        "\n",
        "    # return 30 most frequent chunks\n",
        "    return chunk_counter.most_common(30)\n"
      ],
      "metadata": {
        "id": "LT5QPMDYW_Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking verb phrases\n",
        "from nltk import RegexpParser\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "from vp_chunk_counter import vp_chunk_counter\n",
        "\n",
        "# define verb phrase chunk grammar here\n",
        "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
        "#chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# create a list to hold verb-phrase chunked sentences\n",
        "vp_chunked_oz = list()\n",
        "\n",
        "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
        "for pos_tagged_sentence in pos_tagged_oz:\n",
        "  # chunk each sentence and append to vp_chunked_oz here\n",
        "  vp_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
        "\n",
        "# store and print the most common vp-chunks here\n",
        "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
        "print(most_common_vp_chunks)"
      ],
      "metadata": {
        "id": "xuYAg-1KXNq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunk Filtering"
      ],
      "metadata": {
        "id": "b8pPjDOyXxhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser, Tree\n",
        "from pos_tagged_oz import pos_tagged_oz\n",
        "\n",
        "# define chunk grammar to chunk an entire sentence together\n",
        "grammar = \"Chunk: {<.*>+}\"\n",
        "\n",
        "# create RegexpParser object\n",
        "parser = RegexpParser(grammar)\n",
        "\n",
        "# chunk the pos-tagged sentence at index 230 in pos_tagged_oz\n",
        "chunked_dancers = parser.parse(pos_tagged_oz[230])\n",
        "print(chunked_dancers)\n",
        "\n",
        "# define noun phrase chunk grammar using chunk filtering here\n",
        "chunk_grammar = \"\"\"NP: {<.*>+}\n",
        "                       }<VB.?|IN>+{\"\"\"\n",
        "\n",
        "# create RegexpParser object here\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# chunk and filter the pos-tagged sentence at index 230 in pos_tagged_oz here\n",
        "filtered_dancers = chunk_parser.parse(pos_tagged_oz[230])\n",
        "print(filtered_dancers)\n",
        "\n",
        "# pretty_print the chunked and filtered sentence here\n",
        "Tree.fromstring(str(filtered_dancers)).pretty_print()"
      ],
      "metadata": {
        "id": "_WIfnYjrXs1S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}